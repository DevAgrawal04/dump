{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from captum.attr import IntegratedGradients, visualization\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load your model\n",
    "model = mlflow.pytorch.load_model(\"runs:/your_run_id/your_model_path\")\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "# Determine the device\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Prepare the model for explanability\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "# Custom forward function for IntegratedGradients\n",
    "def forward_func(inputs):\n",
    "    input_ids = inputs[0].long()\n",
    "    attention_mask = inputs[1].long()\n",
    "    return wrapped_model(input_ids, attention_mask)\n",
    "\n",
    "# Initialize Integrated Gradients with custom forward function\n",
    "ig = IntegratedGradients(forward_func)\n",
    "\n",
    "# Function to explain predictions\n",
    "def explain_prediction(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    print(\"Input IDs shape:\", input_ids.shape)\n",
    "    print(\"Input IDs dtype:\", input_ids.dtype)\n",
    "    print(\"Attention Mask shape:\", attention_mask.shape)\n",
    "    print(\"Attention Mask dtype:\", attention_mask.dtype)\n",
    "    \n",
    "    # Run a forward pass to check if the model works\n",
    "    with torch.no_grad():\n",
    "        outputs = wrapped_model(input_ids, attention_mask)\n",
    "    print(\"Model output shape:\", outputs.shape)\n",
    "    \n",
    "    # Now try to get attributions\n",
    "    try:\n",
    "        attributions = ig.attribute((input_ids, attention_mask), target=1, n_steps=50)\n",
    "        print(\"Attribution shape:\", attributions[0].shape)\n",
    "    except Exception as e:\n",
    "        print(\"Error during attribution:\", str(e))\n",
    "        attributions = None\n",
    "    \n",
    "    return attributions, input_ids\n",
    "\n",
    "# Function to visualize attributions\n",
    "def visualize_attributions(text, attributions, input_ids):\n",
    "    if attributions is None:\n",
    "        print(\"No attributions to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Convert attributions to word-level\n",
    "    word_attributions = attributions[0].sum(dim=-1).squeeze(0)\n",
    "    word_attributions = word_attributions / torch.norm(word_attributions)\n",
    "    word_attributions = word_attributions.cpu().detach().numpy()\n",
    "\n",
    "    # Decode tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Remove padding tokens\n",
    "    tokens = [token for token in tokens if token != '[PAD]']\n",
    "    word_attributions = word_attributions[:len(tokens)]\n",
    "\n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(20, 2))\n",
    "    visualization.visualize_text_attr(word_attributions, tokens, ax=ax)\n",
    "    plt.title(\"Integrated Gradients Attribution\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print attributions\n",
    "    for token, attribution in zip(tokens, word_attributions):\n",
    "        print(f\"{token}: {attribution:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "text = \"The CRISPR-Cas9 system has revolutionized gene editing techniques in molecular biology.\"\n",
    "attributions, input_ids = explain_prediction(text)\n",
    "visualize_attributions(text, attributions, input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code\n",
    "Dimensionality Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Captum if not already installed\n",
    "!pip install captum\n",
    "\n",
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "\n",
    "# Function to visualize token attributions\n",
    "def visualize_token_attributions(input_text, attributions, tokenizer):\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    # Normalize attributions for better visualization\n",
    "    attributions = (attributions - np.min(attributions)) / (np.max(attributions) - np.min(attributions) + 1e-8)\n",
    "\n",
    "    # Display tokens with their attribution scores\n",
    "    for token, score in zip(tokens, attributions[:len(tokens)]):\n",
    "        print(f\"{token}: {score:.4f}\")\n",
    "\n",
    "# Function to compute integrated gradients\n",
    "def compute_integrated_gradients(model, tokenizer, input_text, label, max_len=256, baseline_text=\"[PAD]\", n_steps=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input and baseline text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Convert input IDs and attention mask to LongTensor to match model's embedding layer requirements\n",
    "    input_ids = inputs['input_ids'].to(device).type(torch.long)  # Convert input_ids to LongTensor\n",
    "    attention_mask = inputs['attention_mask'].to(device).type(torch.long)  # Convert attention_mask to LongTensor\n",
    "\n",
    "    # Generate a baseline that matches input shape, usually padded zeros or \"[PAD]\"\n",
    "    baseline_ids = tokenizer.encode(\n",
    "        baseline_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device).type(torch.long)  # Convert baseline_ids to LongTensor\n",
    "\n",
    "    # Initialize Integrated Gradients object\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "\n",
    "    # Define a forward function for the Integrated Gradients\n",
    "    def forward_func(input_ids, attention_mask):\n",
    "        # Forward pass through the model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.squeeze(1)  # Adjust this as per your model's output shape\n",
    "\n",
    "    # Compute attributions using integrated gradients\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=input_ids,\n",
    "        baselines=baseline_ids,\n",
    "        target=label,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        n_steps=n_steps,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    # Visualize attributions\n",
    "    print(f\"Integrated Gradients Delta: {delta.item():.4f}\")\n",
    "    visualize_token_attributions(input_text, attributions, tokenizer)\n",
    "\n",
    "# Example usage: Apply IG to a specific test sample\n",
    "sample_text = \"Replace this with an example text from your dataset.\"\n",
    "true_label = 1  # Replace with the correct label for this example\n",
    "\n",
    "# Call the function with the sample text\n",
    "compute_integrated_gradients(model, tokenizer, sample_text, true_label)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
