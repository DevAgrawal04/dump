{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from captum.attr import IntegratedGradients, visualization\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load your model\n",
    "model = mlflow.pytorch.load_model(\"runs:/your_run_id/your_model_path\")\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "# Determine the device\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Model wrapper to debug tensor types\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Ensure that input_ids are always LongTensor\n",
    "        if input_ids.dtype != torch.long:\n",
    "            input_ids = input_ids.long()\n",
    "        print(f\"Inside model - input_ids dtype: {input_ids.dtype}, attention_mask dtype: {attention_mask.dtype}\")\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "# Custom forward function for Integrated Gradients\n",
    "def forward_func(input_ids, attention_mask=None):\n",
    "    return wrapped_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Initialize Integrated Gradients with custom forward function\n",
    "ig = IntegratedGradients(forward_func)\n",
    "\n",
    "# Function to explain predictions\n",
    "def explain_prediction(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Ensure input_ids and attention_mask are of type torch.LongTensor\n",
    "    input_ids = inputs[\"input_ids\"].to(device).long()\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device).long()\n",
    "    \n",
    "    # Check input tensor types before passing to the model\n",
    "    print(\"Before model input_ids dtype:\", input_ids.dtype)\n",
    "    print(\"Before model attention_mask dtype:\", attention_mask.dtype)\n",
    "    \n",
    "    # Run a forward pass to check if the model works\n",
    "    with torch.no_grad():\n",
    "        outputs = wrapped_model(input_ids, attention_mask)\n",
    "    print(\"Model output shape:\", outputs.shape)\n",
    "    \n",
    "    # Now try to get attributions\n",
    "    try:\n",
    "        attributions = ig.attribute(input_ids, additional_forward_args=(attention_mask,), target=0, n_steps=50)\n",
    "        print(\"Attribution shape:\", attributions.shape)\n",
    "    except Exception as e:\n",
    "        print(\"Error during attribution:\", str(e))\n",
    "        attributions = None\n",
    "    \n",
    "    return attributions, input_ids\n",
    "\n",
    "# Function to visualize attributions\n",
    "def visualize_attributions(text, attributions, input_ids):\n",
    "    if attributions is None:\n",
    "        print(\"No attributions to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Convert attributions to word-level\n",
    "    word_attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    word_attributions = word_attributions / torch.norm(word_attributions)\n",
    "    word_attributions = word_attributions.cpu().detach().numpy()\n",
    "\n",
    "    # Decode tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Remove padding tokens\n",
    "    tokens = [token for token in tokens if token != '[PAD]']\n",
    "    word_attributions = word_attributions[:len(tokens)]\n",
    "\n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(20, 2))\n",
    "    visualization.visualize_text_attr(word_attributions, tokens, ax=ax)\n",
    "    plt.title(\"Integrated Gradients Attribution\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print attributions\n",
    "    for token, attribution in zip(tokens, word_attributions):\n",
    "        print(f\"{token}: {attribution:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "text = \"The CRISPR-Cas9 system has revolutionized gene editing techniques in molecular biology.\"\n",
    "attributions, input_ids = explain_prediction(text)\n",
    "visualize_attributions(text, attributions, input_ids)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
