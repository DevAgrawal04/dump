{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the GloVe Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `model` is your trained LSTM model from the GloVe code\n",
    "explainer = shap.DeepExplainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot the SHAP values for a single prediction\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], X_test[0])\n",
    "\n",
    "# To visualize overall feature importance:\n",
    "shap.summary_plot(shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=[\"Non-MI Incident\", \"MI Incident\"])\n",
    "\n",
    "def predict_proba(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=maxlen)\n",
    "    return model.predict(padded)\n",
    "\n",
    "# Explain a single instance\n",
    "i = 0  # index of the test example you want to explain\n",
    "exp = explainer.explain_instance(X_test[i], predict_proba, num_features=10)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SciBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Create a SHAP explainer for Transformer models\n",
    "explainer = shap.Explainer(model, masker=tokenizer)\n",
    "\n",
    "# Explain a single prediction\n",
    "shap_values = explainer(X_test[:10])  # Explain the first 10 samples\n",
    "shap.plots.text(shap_values[0])  # Show explanation for the first text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install captum\n",
    "\n",
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Prepare the data\n",
    "input_text = \"Example text to interpret\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "input_ids = input_ids.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize Integrated Gradients\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# Compute attributions\n",
    "attributions, delta = ig.attribute(inputs=input_ids, target=0, return_convergence_delta=True)\n",
    "\n",
    "# Visualize attributions\n",
    "attributions_sum = attributions.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).cpu().detach().numpy())\n",
    "\n",
    "# Print tokens with their attributions\n",
    "for token, attr in zip(tokens, attributions_sum):\n",
    "    print(f\"{token}: {attr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
